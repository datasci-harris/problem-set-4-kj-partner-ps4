---
Author: Kaijie Wu & Griffin Sharps
title: "Problem Set 4"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
The variables I pulled are: "PRVDR_CTGRY_SBTYP_CD", "PRVDR_CTGRY_CD", "FAC_NAME", "PRVDR_NUM", "PGM_TRMNTN_CD", and "ZIP_CD".

2. 
    a. The unprocesssed data tells us that there were 7245 short-term hospitals in the US in 2016. However, there are 141557 unique entries in this dataset. Given that the American Hospital Association's report (https://www.aha.org/system/files/2018-02/2018-aha-hospital-fast-facts_0.pdf) lists there as only being 5534 hospitals in country in 2016, this data needs to be processed and cleaned to be useful.
    b. Accoding to the American Hospital Association's report , there were 4840 community hospitals in 2016. Because the report states that short-term hospitals are a subcategory of community hospitals, it does not make any sense of there to be more short-term hospitals than community hospitals. We suspect that this differs because the way hospitals close is not appropriately tracked. Closed and inactive hospitals are probably included in the unprocessed data.

```{python}
### importing libraries 
import pandas as pd
import altair as alt
import geopandas as gpd
import altair as alt
import json
from dbfread import DBF
```

```{python}
### paths
# setting base path
base_path = "/Users/griffinsharps/Documents/Repos/student30538/problem_sets/ps4/problem-set-4-kj-partner-ps4/pos{year}.csv"

# Create file paths for different years
file_path_2016 = base_path.format(year=2016)
file_path_2017 = base_path.format(year=2017)
file_path_2018 = base_path.format(year=2018)
file_path_2019 = base_path.format(year=2019)
```

```{python}
### importing 2016 Provider of Service data
pos2016_data = pd.read_csv(file_path_2016)

# filtering for short-term hospitals
short_term_hospitals_2016 = pos2016_data[
    (pos2016_data['PRVDR_CTGRY_CD'] == 1) & 
    (pos2016_data['PRVDR_CTGRY_SBTYP_CD'] == 1)
]

# counting and printing short-term hospitals
num_short_term_hospitals_2016 = short_term_hospitals_2016.shape[0]
print(num_short_term_hospitals_2016)

# counting an printing total hospitals
num_total_hospitals = pos2016_data['PRVDR_NUM'].nunique()
print(num_total_hospitals)
```

3. 
The number of short-term hospitals in 2017 was 7260. In 2018 it was 7277. And in 2019 it was 7303.
```{python}
### 2017 Data
pos2017_data = pd.read_csv(file_path_2017)

short_term_hospitals_2017 = pos2017_data[(pos2017_data['PRVDR_CTGRY_CD'] == 1) & (pos2017_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]

num_short_term_hospitals_2017 = short_term_hospitals_2017.shape[0]
print(num_short_term_hospitals_2017)

### 2018 Data
pos2018_data = pd.read_csv(file_path_2018, encoding='latin1')

short_term_hospitals_2018 = pos2018_data[(pos2018_data['PRVDR_CTGRY_CD'] == 1) & (pos2018_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]

num_short_term_hospitals_2018 = short_term_hospitals_2018.shape[0]
print(num_short_term_hospitals_2018)

### 2019 Data
pos2019_data = pd.read_csv(file_path_2019, encoding='latin1')

short_term_hospitals_2019 = pos2019_data[(pos2019_data['PRVDR_CTGRY_CD'] == 1) & (pos2019_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]

num_short_term_hospitals_2019 = short_term_hospitals_2019.shape[0]
print(num_short_term_hospitals_2019)
```

```{python}
### plot of number of short-term hospitals by year
yearly_counts_df = pd.DataFrame({
    'Year': [2016, 2017, 2018, 2019],
    'Count': [7245, 7260, 7277, 7303]  
})

chart = alt.Chart(yearly_counts_df).mark_line(point=alt.OverlayMarkDef(size=80, color='blue')).encode(
    x=alt.X('Year:O', title='Year', axis=alt.Axis(labelAngle=0)),
    y=alt.Y('Count:Q', title='Number of Short-Term Hospitals', scale=alt.Scale(domain=[7200, 7400])),
    tooltip=[alt.Tooltip('Year', title='Year'), alt.Tooltip('Count', title='Hospitals')]
).properties(
    title="Number of Short-Term Hospitals by Year (2016-2019)",
    width=600,
    height=400
).configure_line(
    size=3
).configure_title(
    fontSize=16,
    anchor='middle',
)

chart.display()
```

4. 
    a. 
```{python}
## checking for duplicates

# Dictionary of file paths for each year
file_paths = {
    "2016": file_path_2016,
    "2017": file_path_2017,
    "2018": file_path_2018,
    "2019": file_path_2019
}

short_term_hospitals_per_year = {}

for year, file_path in file_paths.items():
    data = pd.read_csv(file_path, encoding='latin1')  
    short_term_hospitals = data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    short_term_hospitals_per_year[year] = short_term_hospitals['PRVDR_NUM'].nunique()  

short_term_hospitals_df = pd.DataFrame(list(short_term_hospitals_per_year.items()), columns=['Year', 'Short-Term Hospitals'])

chart = alt.Chart(short_term_hospitals_df).mark_line(point=True).encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Short-Term Hospitals:Q', title='Short-Term Hospitals', scale=alt.Scale(domain=[7200,7400])), 
    tooltip=['Year', 'Short-Term Hospitals']
).properties(
    title='Number of Short-Term Hospitals per Year',
    width=500,
    height=300
)
chart.display()
```
    b. This and the previous plot are the same. This means that the number of short-term hospitals in both datasets is the same. This allows us to establish that no hospitals are duplicated in our datasets.


## Identify hospital closures in POS file (15 pts) (*)

1. 
The suspected number of hospital closures is 174.
```{python}
# Add a 'Year' column to each DataFrame to indicate the year of the data
short_term_hospitals_2016.loc[:, 'Year'] = 2016
short_term_hospitals_2017.loc[:, 'Year'] = 2017
short_term_hospitals_2018.loc[:, 'Year'] = 2018 
short_term_hospitals_2019.loc[:, 'Year'] = 2019

# Combine the DataFrames for all years into a single DataFrame
df_pos_all_years = pd.concat([short_term_hospitals_2016, short_term_hospitals_2017, short_term_hospitals_2018, short_term_hospitals_2019], ignore_index=True)

# Identify hospitals that were active in 2016 (i.e. 'PGM_TRMNTN_CD' equals 0)
hospitals_2016_active = df_pos_all_years[(df_pos_all_years['Year'] == 2016) & (df_pos_all_years['PGM_TRMNTN_CD'] == 0)]

# Create an empty DataFrame to store information about suspected hospital closures
hospital_closure = pd.DataFrame(columns=['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD', 'Year of Closure'])

# Iterate through each year from 2017 to 2019 to check for hospital closures
for year in range(2017, 2020):
     # Get a set of provider numbers for hospitals that are still active in the current year
    active_hospitals_current_year = set(df_pos_all_years[(df_pos_all_years['Year'] == year) & 
                                                   (df_pos_all_years['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM'])
     # Initialize an empty DataFrame to store closure information for the current year
    year_closures = pd.DataFrame()
    # Iterate over each hospital that was active in 2016
    for _, hospital_info in hospitals_2016_active.iterrows():
        # Check if the hospital is no longer active in the current year
        if hospital_info['PRVDR_NUM'] not in active_hospitals_current_year:
            # Create a DataFrame entry for the hospital closure
            closure_entry = pd.DataFrame({
                'PRVDR_NUM': [hospital_info['PRVDR_NUM']],
                'FAC_NAME': [hospital_info['FAC_NAME']],
                'ZIP_CD': [hospital_info['ZIP_CD']],
                'Year of Closure': [year]
            })
            # Append the closure entry to the 'year_closures' DataFrame
            year_closures = pd.concat([year_closures, closure_entry], ignore_index=True)

    # Append the year's closure information to the main 'hospital_closure' DataFrame
    hospital_closure = pd.concat([hospital_closure, year_closures], ignore_index=True)

# Remove duplicate entries based on 'PRVDR_NUM' to ensure each hospital appears only once
hospital_closure.drop_duplicates(subset='PRVDR_NUM', inplace=True)

print(hospital_closure)
print(f"Total suspected closures: {len(hospital_closure)}")
```

2.  
```{python}
sorted_hospitals = hospital_closure.sort_values(by='FAC_NAME').head(10)

print(sorted_hospitals[['FAC_NAME', 'Year of Closure']])
```

3. 
    a.
The number of potential merger/acquisition is 148.
```{python}
# List to store CMS certification numbers of hospitals that might have been involved in a merger or acquisition
potential_mergers = []

# Iterate through each row in the hospital_closure DataFrame
for index, row in hospital_closure.iterrows():
    # Get the ZIP code and the year of suspected closure for the current hospital
    zip_code = row['ZIP_CD']
    closure_year = row['Year of Closure']

    # Get data for the year after the suspected closure for the same ZIP code
    # Filter for hospitals that are still active (PGM_TRMNTN_CD == 0)
    next_year_data = df_pos_all_years[(df_pos_all_years['Year'] == closure_year + 1) & 
                                (df_pos_all_years['ZIP_CD'] == zip_code) & 
                                (df_pos_all_years['PGM_TRMNTN_CD'] == 0)]

    # Get data for the year of the suspected closure for the same ZIP code
    current_year_data = df_pos_all_years[(df_pos_all_years['Year'] == closure_year) & 
                                   (df_pos_all_years['ZIP_CD'] == zip_code) & 
                                   (df_pos_all_years['PGM_TRMNTN_CD'] == 0)]

    # If the number of active hospitals in the next year is not less than the current year,
    # it suggests a potential merger or acquisition, so we add the hospital to potential_mergers
    if len(next_year_data) >= len(current_year_data):
        potential_mergers.append(row['PRVDR_NUM'])

# Filter out hospitals identified as potential mergers from hospital_closure
corrected_closures = hospital_closure[~hospital_closure['PRVDR_NUM'].isin(potential_mergers)]

# Count the number of potential mergers/acquisitions
mergers_count = len(potential_mergers)
print(f"Number of potential mergers/acquisitions: {mergers_count}")
```

    b.
The number of corrected suspected hospital closures is 26.
```{python}
# Calculate and print the number of hospitals remaining after removing potential mergers
remaining_hospitals_count = len(corrected_closures)
print(f"Remaining suspected closures after correction: {remaining_hospitals_count}")
```

    c.
```{python}
# Sort the corrected closures by facility name and display the first 10 entries
sorted_corrected_hospitals = corrected_closures.sort_values(by='FAC_NAME').head(10)
print(sorted_corrected_hospitals[['FAC_NAME', 'Year of Closure']])
```

## Download Census zip code shapefile (10 pt) 

1. 
    a.
The five file types are Shape File (.shp), Shape Index File (.shx), Attribute Data File (.dbf), Projection File (.prj), and Metadata File (.xml).

* A Shape File contains the geometry data:the shapes of geographical features. 
* A Shape Index File is an index for that geometry data.
* An Attribute Data File holds attribute data in a dBase table format, where each row corresponds to a * shape in the shape file, providing details such as ZIP code and other metadata.
* A Projection File is the information about the coordinate system and projection used. It is critical for accurately aligning shapefile data on a map.
* A Metadata File provides the information about the shapefile, including its origin, date of creation, and any limitations on its usage.

    b. 
By looking at the files we can see that:
* The size of Attribute Data File is 6.4 MB.
* The size of Projection File is 165 bytes.
* The size of Shape File is 837.5 MB.
* The size of Shape Index File is 266 KB.
* The size of Metadata File is 16 KB.

2. 

```{python}
# Load the ZIP Code Shapefile using GeoPandas
zip_shapefile = gpd.read_file('/Users/griffinsharps/Documents/Repos/student30538/problem_sets/ps4/US 860 Census 2010/gz_2010_us_860_00_500k.shp')

# Use a range to generate the prefixes and convert them to strings
texas_prefixes = tuple(str(i) for i in range(75, 80))

# Subset the GeoDataFrame using the generated prefixes
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(texas_prefixes)].copy()

# Filter the POS data for the year 2016
pos_2016 = df_pos_all_years[(df_pos_all_years['Year'] == 2016)]

# Count the number of hospitals per ZIP code and create a DataFrame
hospital_counts = pos_2016['ZIP_CD'].value_counts().reset_index()
hospital_counts.columns = ['ZIP_CD', 'Hospital_Count']  # Rename columns for clarity

# Make a copy of the filtered GeoDataFrame to avoid warnings
texas_zip_shapefile = texas_zip_shapefile.copy()

# Convert 'ZCTA5' to integer format for merging
texas_zip_shapefile['ZIP_CD'] = texas_zip_shapefile['ZCTA5'].astype(int)

# Merge the Texas ZIP shapefile data with the hospital counts DataFrame
# This adds the 'Hospital_Count' column to the GeoDataFrame
texas_map_data = texas_zip_shapefile.merge(hospital_counts, how='left', left_on='ZIP_CD', right_on='ZIP_CD')

# Fill any NaN values in 'Hospital_Count' with 0
texas_map_data['Hospital_Count'] = texas_map_data['Hospital_Count'].fillna(0)

# Convert the merged GeoDataFrame to GeoJSON format for visualization in Altair
texas_geojson = json.loads(texas_map_data.to_json())

# Create a choropleth map using Altair to visualize the number of hospitals per ZIP code
choropleth = alt.Chart(alt.Data(values=texas_geojson['features'])).mark_geoshape().encode(
    color=alt.Color('properties.Hospital_Count:Q', scale=alt.Scale(scheme='blues'), title='Number of Hospitals'),
    tooltip=[
        alt.Tooltip('properties.ZCTA5CE10:O', title='ZIP Code'),  
        alt.Tooltip('properties.Hospital_Count:Q', title='Hospital Count')
    ]
).transform_calculate(
    ZCTA5CE10='datum.properties.ZCTA5CE10',
    Hospital_Count='datum.properties.Hospital_Count'
).properties(
    width=600,  
    height=400,  
    title='Number of Hospitals per ZIP Code in Texas (2016)'
)

choropleth.display()

# This yields a skewed map of the state. ChatGPT suggests that this because we did not use a "geographic projection of the data". We corrected this using the albersUSA projection, which is built into Altair.

# Create a choropleth map using Altair with the 'albersUsa' projection
choropleth = alt.Chart(alt.Data(values=texas_geojson['features'])).mark_geoshape().encode(
    color=alt.Color('properties.Hospital_Count:Q', scale=alt.Scale(scheme='blues'), title='Number of Hospitals'),
    tooltip=[
        alt.Tooltip('properties.ZCTA5CE10:O', title='ZIP Code'),  
        alt.Tooltip('properties.Hospital_Count:Q', title='Hospital Count')
    ]
).transform_calculate(
    ZCTA5CE10='datum.properties.ZCTA5CE10',
    Hospital_Count='datum.properties.Hospital_Count'
).properties(
    width=600,  
    height=400,  
    title='Number of Hospitals per ZIP Code in Texas (2016)'
).project(
    type='albersUsa'  # Apply the Albers USA projection
)

choropleth.display()
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1.  

```{python}
zips_all_centroids = zip_shapefile.copy()  # Make a copy to avoid modifying the original GeoDataFrame

# Update the 'geometry' column to contain the centroids of each polygon
zips_all_centroids['geometry'] = zips_all_centroids['geometry'].centroid

# Display the dimensions of the resulting GeoDataFrame
print(zips_all_centroids.shape)
print(zips_all_centroids.columns)
```
The dimensions of the resulting GeoDataFrame are 33120 x 6.

A manual search of the .xml file shows that the columns are:
* GEO_ID: Unique identifier for a geographic entity [alphanumeric]
* ZCTA5: ZIP Code Tabulation Area [5-digit Census code]
* Name: Name without translated Legal/Statistical Area Description (LSAD) [alphanumeric]
* LSAD: Standard abbreviation translation of Legal/Statistical Area Description (LSAD) code as used on census maps [alpha]
* CENSUSAREA: Area of entity before generalization in square miles [numeric]
* geometry: the geometry of the centroid points

2. 

```{python}
# GeoDataFrame for all Texas zip codes
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith((texas_prefixes))].copy()

# New tuple with all zip code prefixes
all_prefixes = tuple(
    str(i) for i in range(75, 80)   # 75-79
) + tuple(
    str(i) for i in range(870, 885) # 870-884
) + tuple(
    str(i) for i in range(73, 75)   # 73-74
) + tuple(
    str(i) for i in range(716, 730) # 716-729
) + tuple(
    str(i) for i in range(700, 716) # 700-715
)

# Subset the GeoDataFrame using the combined prefixes
zips_texas_borderstates_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith(all_prefixes)].copy()

print(zips_texas_centroids.shape)
print(zips_texas_borderstates_centroids.shape)

zips_texas_borderstates_centroids
```
There are 1935 zip codes in Texas and 4057 in Texas and its bordering states.

3. 
```{python}
# Convert 'ZIP_CD' to a string in the hospital_counts DataFrame to match the ZCTA5 column
hospital_counts['ZIP_CD'] = hospital_counts['ZIP_CD'].astype(int).astype(str)

# Merge the hospital_counts DataFrame with the zips_texas_borderstates_centroids GeoDataFrame
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospital_counts, 
    how='left', 
    left_on='ZCTA5', 
    right_on='ZIP_CD'
)

# Fill any NaN values in the Hospital_Count column with 0
zips_withhospital_centroids['Hospital_Count'] = zips_withhospital_centroids['Hospital_Count'].fillna(0)

# Subset the merged GeoDataFrame to only include ZIP codes with at least one hospital
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['Hospital_Count'] > 0]

zips_withhospital_centroids
```
I used a left merge and merged on the variables ZCTA5 (left) and ZIP_CD (right). This ensures that we keep all of the zip codes in the zips_texas_borderstates_centroid GeoDataFrame.

4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
